# 计算机组成原理

## 1.计算机组成

- 计算机应用程序（应用软件）
- 操作系统、编译原理、计算机网络
- 计算机体系结构（系统软件）
- 计算机组成
- `CPU/GPU`、内存/硬盘、显示器/键盘（硬件）

## 2.计算机三大件

1. `CPU`(`Central Processing Unit`)
   - CPU 是计算机最昂贵的部分
   - CPU 是一个超级精细的印刷电路板
2. 内存（`Memory`）
   - 撰写的程序、打开的浏览器、运行的游戏，都要加载到内存里才能运行
   - 程序读取的数据、计算得到的结果，也都要放在内存里
   - 内存越大，能加载的东西自然也就越多。
   - 内存通常直接可以插在主板上
   - 存放在内存里的程序和数据，需要被 CPU 读取，CPU 计算完之后，还要把数据写回到内存。
3. 主板（`Motherboard`）
   - `CPU` 要插在主板上，内存也要插在主板上
   - 主板的芯片组（`Chipset`）和总线（`Bus`）解决了 `CPU` 和内存之间如何通信的问题
   - 芯片组控制了数据传输的流转，也就是数据从哪里到哪里的问题
   - 总线则是实际数据传输的高速公路
   - 总线速度（Bus Speed）决定了数据能传输得多快
4. 其他
   - `I/O`设备，显示器、鼠标、键盘，`I/O`设备通过主板上的南桥芯片来和`CPU`通信
   - 硬盘将数据持久存储下来
   - 显卡，显卡里存在`GPU`（`Graphics Processing Unit，图形处理器`）
   - 网吧没有硬盘，通过局域网读写远程网络硬盘里面的数据
   - 云服务通过`ssh`远程登陆，没必要配显示器、鼠标、键盘
5. 计算机祖师爷之一冯·诺依曼（John von Neumann）提出的冯·诺依曼体系结构（Von Neumannarchitecture），也叫存储程序计算机。

## 3.计算机组成原理

1. 计算机硬件组成部分
   - 运算器、控制器、存储器、输入设备和输出设备
2. 计算器 2 个核心指标
   - 性能和功耗
3. 计算机的指令和计算
   - 程序通过编译器和汇编器，变成机器指令
   - 指令的执行是由控制器控制
   - 运算器，算术逻辑单元

## 4.CPU 主频

1. 性能
   - 响应时间：执行程序需要花费的时间
   - 吞吐率：时间范围内能处理多少事情，如网络带宽，计算器 8 核，16 核
2. CPU 主频
   - `2.8GHz`表示 `CPU` 在 1s 时间内，可以执行的简单指令数量是 `2.8G` 条
3. CPU 的执行时间
   - 程序的 `CPU` 执行时间=`CPU` 时钟周期数 x 时钟周期时间
   - `CPU` 时钟周期时间：1/2.8G
   - `CPU` 时钟周期数：指令数 x 每条指令的平均时钟周期数（`Cycles Per Instruction`简称`CPI`）
   - 加法和乘法都对应着一条 `CPU` 指令，但是乘法需要的 `Cycles` 就比加法要多，自然也就慢
4. 超频
   - 将 `CPU` 内部的钟调快，执行更快，散热压力更大

## 5.功耗

1. CPU（超大规模集成电路）
   - 这些电路实际是由晶体管组合而成的，CPU 计算时，其实就是让晶体管里面的开关不断地打开和关闭，来组合完成各种运算和功能
2. 计算更快
   - 多放一些晶体管，增加密度
   - 让晶体管打开和关闭更快，提升主频
3. 以上 2 点，都会增加功耗，带来耗电和散热问题
4. 在 CPU 上抹硅脂、装风扇、以及用上水冷来散热
5. 功耗：功耗 ~= 1/2 × 负载电容 × 电压的平方 × 开关频率 × 晶体管数量
6. 通过并行提升性能：2 核、4 核、8 核
7. 优化后的执行时间 = 受优化影响的执行时间 / 加速倍数 + 将多个计算结果汇总的时间

## 6.计算机指令：让我们试试用纸带编程

1. `CPU` 就是一个执行各种计算机指令（`Instruction Code`）的逻辑机器
2. 上世纪 60、70 年代，将穿孔卡交给计算机执行
3. 程序执行过程

   1. 编译（Compile）成汇编代码。
   2. 针对汇编代码，我们可以再用汇编器（Assembler）翻译成机器码（Machine Code）
   3. 这一条条机器码，就是一条条的计算机指令。
   4. 这样一串串的 16 进制数字，就是我们 CPU 能够真正认识的计算机指令。
   5. 汇编码是 add、mov 这些用英文表示的指令
   6. 机器码是 8b 45 f8 的 16 进制数字

4. 指令
   1. 算术类指令：我们的加减乘除，在 CPU 层面，都会变成一条条算术类指令。
   2. 数据传输类指令: 给变量赋值、在内存里读写数据，用的都是数据传输类指令。
   3. 逻辑类指令: 逻辑上的与或非，都是这一类指令。
   4. 条件分支类指令: 日常我们写的“if/else”，其实都是条件分支类指令。
   5. 无条件跳转指令: 写一些大一点的程序，我们常常需要写一些函数或者方法。
5. 指令表格
   |指令类型|示例指令|示例汇编代码|含义|注释|
   |:----|:-----|:-----|:----|:-----|
   |算术类指令|add|add$s1,$s2,$s3|$s1=$s2+$s3|将 s2 和 s3 寄存器中的数相加后的结果放到寄存器 s1 中|
   |逻辑类指令|or|or$s1,$s2,$s3|$s1=$s2|$s3|将 s2 和 s3 寄存器中的数按位取或的结果放到寄存器 s1 中|
   |数据传输指令|load work|load $s1,10($s2)|$s1=memory[$s2+10]|取 s2 寄存器中的数，加上 10 的偏移量后，找到内存中的字，存入到 s1 寄存器中|
   |条件分支指令|branch on equal|beq $s1,$s2,10|if($s1==$s2)go to PC+4+10|如果 s1 和 s2 寄存器内的值相等，从程序计数器往后跳 10|
   |无条件跳转指令|jump|j 1000|go to 1000|跳转到 1000 这个目标地址|

## 7.指令跳转：原来 if...else 就是 goto

1. 我们在单条指令的基础上，学习了程序里的多条指令，究竟是怎么样一条一条被执
   行的。除了简单地通过 PC 寄存器自增的方式顺序执行外，条件码寄存器会记录下当前执行
   指令的条件判断状态，然后通过跳转指令读取对应的条件码，修改 PC 寄存器内的下一条指
   令的地址，最终实现 if…else 以及 for/while 这样的程序控制流程。
2. 我们可以用高级语言，可以用不同的语法，比如 if…else 这样的条件分支，
   或者 while/for 这样的循环方式，来实现不用的程序运行流程，但是回归到计算机可以识别
   的机器指令级别，其实都只是一个简单的地址跳转而已，也就是一个类似于 goto 的语句。
3. 在硬件层面实现这个 goto 语句，除了本身需要用来保存下一条指令地址，以及当前正
   要执行指令的 PC 寄存器、指令寄存器外，我们只需要再增加一个条件码寄存器，来保留条
   件判断的状态。这样简简单单的三个寄存器，就可以实现条件判断和循环重复执行代码的功
   能。
4. 三大寄存器
   - PC 寄存器: 指令地址寄存器，用来存放下一条需要执行的计算机指令的内存地址。
   - 指令寄存器：用来存放当前正在执行的指令。
   - 条件码寄存器：用里面的一个一个标记位（Flag），存放 CPU 进行算术或者逻辑计算的结果。

## 8.函数调用：为什么会发生 stack overflow？

1. CPU 指令层面是在程序栈中执行函数调用的
2. 压栈和出栈使程序在不同的函数调用中进行转移
3. 函数内联进行优化
4. 循环调用会导致栈溢出
5. 程序栈相当于一个记忆功能，能在跳转去运行新的指令之后，再回到跳出去的位置，能够实现更加丰富和灵活的指令执行流程。
   函数能使我们复用代码和指令

## 9.ELF 和静态链接：为什么程序无法同时在 Linux 和 Windows 下运行？

1. windows 和 linux 可执行文件的格式不一样
2. Linux 只能执行`ELF`文件，使用开源项目`Wine`可以兼容`PE`文件
3. Windows 只能执行`PE`文件，使用`Windows Subsystem for Linux`可以解析和加载`ELF`格式文件

## 10.程序装载：“640K 内存”真的不够用么？

1. 程序装载面临的挑战
   1. 可执行程序加载后占用的内存空间应该是连续的
   2. 我们需要同时加载很多个程序，并且不能让程序自己规定在内存中加载的位置
      - 虚拟内存地址: 指令里用到的内存地址
      - 物理内存地址: 实际在内存硬件里面的空间地址
   3. 我们维护一个虚拟内存到物理内存的映射表，这样实际程序指令执行的时候，会通过虚拟内存地址，找到对应的物理内存地址，然后执行。
   4. 因为是连续的内存地址空间，所以我们只需要维护映射关系的起始地址和对应的空间大小就可以了。
2. 内存碎片问题
   - 启用程序后再关掉，会导致这个程序占用的内存不连续
   - 使用内存交换处理
3. 内存分页
   - 和分段这样分配一整段连续的空间给到程序相比，分页是把整个物理内存空间切成一段段固定尺寸的大小
   - 而对应的程序所需要占用的虚拟内存空间，也会同样切成一段段固定尺寸的大小。这样一个连续并且尺寸固定的内存空间，我们叫页（Page）。
   - 页的尺寸一般远远小于整个程序的大小。在 Linux 下，我们通常只设置成 4KB。
   - CPU 只需要执行当前的指令，极限情况下，内存也只需要加载一页就好了。再大的程序，也可以分成一页。
   - 硬盘的访问速度比内存慢很多

## 11. 动态链接：程序内部的“共享单车”

1. 在动态链接的过程中，我们想要“链接”的，不是存储在硬盘上的目标文件代码，而是加载到内存中的共享库（Shared Libraries）。
2. 在 Windows 下，这些共享库文件就是.dll（Dynamic-Link Libary） 文件
3. 在 Linux 下，这些共享库文件就是.so （Shared Object）文件
4. 在进行 Linux 下的程序开发的时候，我们一直会用到各种各样的动态链接库。C 语言的标准库就在 1MB 以上。我们撰写任何一个程序可能都需要用到这个库，常见的 Linux 服务器里，/usr/bin 下面就有上千个可执行文件

## 12.二进制编码：“手持两把锟斤拷，口中疾呼烫烫烫”？

1. 原码表示法
   - 把一个数最左侧的一位，当成是对应的正负号，比如 0 为正数，1 为负数
   - 一个 4 位的二进制数， 0011 就表示为 +3。 1011 表示-3
2. ASCII 码（American Standard Code for Information Interchange，美国信息交换标准代码）。
   - 用 8 位二进制中的 128 个不同的数，映射到 128 个不同的字符里。
   - 小写字母 a 在 ASCII 里面，就是第 97 个，也就是二进制的 0110 0001，对应的十六进制表示就是 61
   - 而大写字母 A，就是第 65 个，也就是二进制的 0100 0001，对应的十六进制表示就是 41。
3. 不管是整数也好，浮点数也好，采用二进制序列化会比存储文本省下不少空间。
4. 字符集，表示的可以是字符的一个集合。比如“中文”就是一个字符集
5. 日常说的 Unicode，其实就是一个字符集，包含了 150 种语言的 14 万个不同的字符。
6. 字符编码则是对于字符集里的这些字符，怎么一一用二进制表示出来的一个字典
7. Unicode，就可以用 UTF-8、UTF-16，乃至 UTF-32 来进行编码，存储成二进制
8. “我”对应的 UTF-8 的编码是 6211，也可以自己发明一套编码方式 GT-32,存储为 1126
9. 同样的文本，需采用和存储一致的编码方式进行解码，不然会乱码

## 13 | 理解电路：从电报机到门电路，我们如何做到“千里传信”？

1. 电报传输的信号有两种，一种是短促的点信号（dot 信号），一种是长一点的划信号（dash 信号）
2. 我们把“点”当成“1”，把“划”当成“0”。
3. 电影里最常见的电报信号是“SOS”，这个信号表示出来就是 “点点点划划划点点点”。
4. 电报本质上是通过电信号来进行传播的，所以从输入信号到输出信号基本上没有延时
5. 电报机本质上就是一个“蜂鸣器 + 长长的电线 + 按钮开关”
6. 为了能够实现这样接力传输信号，在电路里面，工程师们造了一个叫作继电器（Relay）的设备。
7. 继电器通电之后，产生磁力，下一级的中继开关被打开，传递信号再到下一个中继
8. 光缆，随着距离的增长、反射次数的增加，信号也会有所衰减，我们同样要每隔一段距离，来增加一个用来重新放大信号的中继。

## 14 | 加法器/乘法器：如何像搭乐高一样搭电路（上）？

1. 电路都是由门电路组成的
   - ![门电路](https://github.com/bearnew/picture/blob/master/markdown_v2/2021/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/%E9%97%A8%E7%94%B5%E8%B7%AF.PNG?raw=true)
2. 今天包含十亿级别晶体管的现代 CPU，都是由这样一个一个的门电路组合而成的。
3. 异或门就是一个最简单的整数加法，所需要使用的基本门电路。
4. 我们把两个门电路打包，给它取一个名字，就叫作半加器（Half Adder）。
5. 我们用两个半加器和一个或门，就能组合成一个全加器。
   - 第一个半加器，我们用和个位的加法一样的方式，得到是否进位 X 和对应的二个数加和后的结果 Y，这样两个输出。
   - 然后，我们把这个加和后的结果 Y，和个位数相加后输出的进位信息 U，再连接到一个半加器上，就会再拿到一个是否进位的信号 V 和对应的加和后的结果 W。
   - 通过两个半加器和一个或门，我们就得到了一个，能够接受进位信号、加数和被加数，这样三个数组成的加法。这就是我们需要的全加器。
6. 硬件分层
   - 门电路->半加器->全加器->加法器->`ALU`(算术逻辑单元)
7. 计算机体系结构中 RISC 和 CISC 的经典历史路线之争
   - 是用更少更简单的电路，但是需要更长的门延迟和时钟周期；（足球赛，32 个球队捉对厮杀，复杂度 O(log N)）
   - 用更复杂的电路，但是更短的门延迟和时钟周期来计算一个复杂的指令（单败淘汰赛，32 个球队，需要 31 场比赛）

## 15 | 浮点数和定点数（上）：怎么用有限的 Bit 表示尽可能多的信息？

1. 浮点数的科学计数法的表示，有一个`IEEE`的标准
   - 用 32 比特表示单精度的浮点数，也就是我们常常说的 float 或者 float32 类型
   - 用 64 比特表示双精度的浮点数，也就是我们平时说的 double 或者 float64 类型
2. 单精度
   |s=符号位|e=指数位|f=有效数位|
   |:----|:-----|:------|
   |1 个比特|8 个比特|23 个比特|
3. 浮点数没有办法精确表示 0.3、0.6 和 0.9。
4. 只有 0.5 能够被精确地表示成二进制的浮点数，也就是 s = 0、e = -1、f = 0 这样的情况
   ```js
   0.5 = (−1)^0 × 1.0 × 2^-1= 0.5
   ```

## 16 | 浮点数和定点数（下）：深入理解浮点数到底有什么用？

1. js 采用 64 位固定长度来表示，也就是标准的 double 双精度浮点数（相关的还有 float 32 位单精度）
   - 符号位 S：第 1 位是正负数符号位（sign），0 代表正数，1 代表负数
   - 指数位 E：中间的 11 位存储指数（exponent），用来表示次方数
   - 尾数位 M：最后的 52 位是尾数（mantissa），超出的部分自动进一舍零
2. 十进制的 0.1 和 0.2 会被转换成二进制的，但是由于浮点数用二进制表示时是无穷的：
   - 0.1 -> 0.0001 1001 1001 1001...( 1100 循环)
   - 0.2 -> 0.0011 0011 0011 0011...( 0011 循环)
3. 因浮点数小数位的限制而截断的二进制数字，再转换为十进制，就成了 0.30000000000000004
4. 浮点数的二进制计算方式

   - 和整数的二进制表示采用“除以 2，然后看余数”的方式相比，小数部分转换成二进制是用
     一个相似的反方向操作，就是乘以 2，然后看看是否超过 1。如果超过 1，我们就记下 1，并把结果减去 1，进一步循环操作。在这里，我们就会看到，0.1 其实变成了一个无限循环的二进制小数，0.000110011。这里的“0011”会无限循环下去。

   - ![calc](https://github.com/bearnew/picture/blob/master/markdown_v2/2021/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/%E6%B5%AE%E7%82%B9%E6%95%B0%E4%BA%8C%E8%BF%9B%E5%88%B6.PNG?raw=true)

5. 9.1 这个十进制数就变成了 1001.000110011…这样一个二进制表示

## 17.建立数据通路（上）：指令+运算=CPU

1. 指令周期

   1. `Fetch(取得指令)`
      - 从 PC 寄存器里找到对应的指令地址，根据指令地址从内存里把具体的指令，加载到指令寄存器中，然后把 PC 寄存器自增，好在未来执行下一条指令。
   2. `Decode(指令译码)`
      - 根据指令寄存器里面的指令，解析成要进行什么样的操作，是 R、I、J 中的哪一种指令，具体要操作哪些寄存器、数据或者内存地址。
   3. `Excute(执行指令)`
      - 实际运行对应的 R、I、J 这些特定的指令，进行算术逻辑操作、数据传输或者直接的地址跳转。

2. 指令由计算机的不同组件完成
   - 指令放在存储器中
   - 由控制器取出指令
   - 控制器再进行解码
   - 算术逻辑单元（ALU）执行指令
3. CPU 需要 4 种基本电路
   - ALU 这样的组合逻辑电路
   - 用来存储数据的锁存器和 D 触发器电路
   - 用来实现 PC 寄存器的计数器电路
   - 用来解码和寻址的译码器电路。
4.

## 18 | 建立数据通路（中）：指令+运算=CPU

1. 时序电路：把数据存储下来
2. 反馈电路：创建时钟信号
3. 利用时钟信号和门电路组合，实现状态记忆

## 19.CPU 实现抽象逻辑

- ![CPU实现抽象逻辑](https://github.com/bearnew/picture/blob/master/markdown_v2/2021/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/CPU%E5%AE%9E%E7%8E%B0%E6%8A%BD%E8%B1%A1%E9%80%BB%E8%BE%91.PNG?raw=true)

## 20.面向流水线的指令设计（上）：一心多用的现代 CPU

1. 为了能够不浪费 CPU 的性能，我们通过把指令的执行过程，切分成一个一个流水线级，来提升 CPU 的吞吐率。

## 21 | 面向流水线的指令设计（下）：奔腾 4 是怎么失败的？

1. 流水线技术和其他技术一样，都讲究一个“折衷”（Trade-Off）。
2. 一个合理的流水线深度，会提升我们 CPU 执行计算机指令的吞吐率。我们一般用 IPC（Instruction Per Cycle）来衡量 CPU 执行指令的效率。
3. 过深的流水线，不仅不能提升计算机指令的吞吐率，更会加大计算的功耗和散热问题。

## 22 丨冒险和预测（一）：hazard 是“危”也是“机”

1. 我们的内存虽然没有按照功能拆分，但是在高速缓存层面进行了拆分，也就是拆分成指令缓存和数据缓存这样的方式，从硬件层面，使得同一个时钟下对于相同资源的竞争不再发生。

## 23 丨冒险和预测（二）：流水线里的接力赛

1. 操作数前推，就是通过在硬件层面制造一条旁路，让一条指令的计算结果，可以直接传输给下一条指令，而不再需要“指令 1 写回寄存器，指令 2 再读取寄存器“这样多此一举的操作
2. 后面的指令可以减少，甚至消除原本需要通过流水线停顿，才能解决的数据冒险问题
3. 通过操作数前推，我们进一步提升了 CPU 的运行效率

## 24 | 冒险和预测（三）：CPU 里的“线程池”

1. 因为数据的依赖关系和指令先后执行的顺序问题，很多时候，流水线不得不“阻塞”在特定的指令上。即使后续别的指令，并不依赖正在执行的指令和阻塞的指令，也不能继续执行。
2. 而乱序执行，则是在指令执行的阶段通过一个类似线程池的保留站，让系统自己去动态调度先执行哪些指令。
3. 动态调度巧妙地解决了流水线阻塞的问题。指令执行的先后顺序，不再和它们在程序中的顺序有关。我们只要保证不破坏数据依赖就好了

## 25 | 冒险和预测（四）：今天下雨了，明天还会下雨么？

1. 在改造我们的 CPU 功能，通过增加对应的电路的方式，来缩短分支带来的延迟。

## 26 | Superscalar 和 VLIW：如何让 CPU 的吞吐率超过 1？

1. 超标量可以让 CPU 不仅在指令执行阶段是并行的，在取指令和指令译码的时候，也是并行的。通过超标量技术，可以使得你所使用的 CPU 的 IPC 超过 1。
2. 在 Intel 的 x86 的 CPU 里，从 Pentium 时代，第一次开始引入超标量技术，整个 CPU 的性能上了一个台阶

## 27 | SIMD：如何加速矩阵乘法？

1. CPU 的“并行计算”方案： 超线程和 SIMD
2. 超线程，其实是一个“线程级并行”的解决方案。它通过让一个物理 CPU 核心，“装作”两个逻辑层面的 CPU 核心，使得 CPU 可以同时运行两个不同线程的指令
3. SIMD 技术，则是一种“指令级并行”的加速方案，一次性取出多条数据，交给 CPU 并行计算。

## 28 丨异常和中断：程序出错了怎么办

| 类型 | 原因         | 示例               | 触发时机 | 处理后操作 |
| :--- | :----------- | :----------------- | :------- | :--------- |
| 中断 | I/O 设备信号 | 用户键盘输入       | 异步     | 下一条指令 |
| 陷阱 | 程序刻意触发 | 程序进入系统调用   | 同步     | 下一条指令 |
| 故障 | 程序执行出错 | 程序加载的缺页错误 | 同步     | 当前指令   |
| 中止 | 故障无法恢复 | ECC 内存校验失败   | 同步     | 退出程序   |

## 29 丨 CISC 和 RISC：为什么手机芯片都是 ARM

1. RISC 的指令是固定长度的，RISC 的指令集里的指令数少，而且单个指令只完成简单的功能，所以被称为“精简”
2. CISC 的指令是可变长度的。CISC 里的指令数多，为了节约内存，直接在硬件层面能够完成复杂的功能，所以被称为“复杂”。
3. RISC 的通过减少 CPI 来提升性能
4. CISC 通过减少需要的指令数来提升性能。
5. Intel 仍然把持着 PC 和服务器市场，但是更多的市场上的 CPU 芯片来自基于 ARM 架构的智能手机了
6. ARM 似乎已经垄断了移动 CPU 市场的时候，开源的 RISC-V 出现了，也给了计算机工程师们新的设计属于自己的 CPU 的机会。

## 30 丨 GPU（上）：为什么玩游戏需要使用 GPU

1. 基于多边形建模的三维图形的渲染过程
   - 经过顶点处理、图元处理、栅格化、片段处理以及像素操作这 5 个步骤
   - 这 5 个步骤把存储在内存里面的多边形数据变成了渲染在屏幕上的画面
   - 因为里面的很多步骤，都需要渲染整个画面里面的每一个像素，所以其实计算量是很大的
2. 像 3dfx 和 NVidia 这样的厂商就推出了 3D 加速卡，用硬件来完成图元处理开始的渲染流程
3. GPU 和现代的显卡还不太一样，它们是用固定的处理流程来完成整个 3D 图形渲染的过程
4. GPU 不用像 CPU 那样考虑计算和处理能力的通用性。我们就可以用比起 CPU 芯片更低的成本，更好地完成 3D 图形的渲染工作。
5. 而 3D 游戏的时代也是从 GPU 出现开始的。

## 31 | GPU（下）：为什么深度学习需要使用 GPU？

1. GPU 一开始是没有“可编程”能力的，程序员们只能够通过配置来设计需要用到的图形渲染效果
2. “可编程管线”的出现，程序员们可以在顶点处理和片段处理去实现自己的算法
3. 为了进一步去提升 GPU 硬件里面的芯片利用率，微软在 XBox 360 里面，第一次引入了“统一着色器架构”，使得 GPU 变成了一个有“通用计算”能力的架构。
4. 我们从一个 CPU 的硬件电路出发，去掉了对 GPU 没有什么用的分支预测和乱序执行电路，来进行瘦身
5. 之后，基于渲染管线里面顶点处理和片段处理就是天然可以并行的了。我们在 GPU 里面可以加上很多个核。
6. GPU 里面的多核、多 ALU，加上多 Context，使得它的并行能力极强。
7. 同样架构的 GPU，如果光是做数值计算的话，算力在同样价格的 CPU 的十倍以上。
8. 强大计算能力，以及“统一着色器架构”，使得 GPU 非常适合进行深度学习的计算模式，也就是海量计算，容易并行，并且没有太多的控制分支逻辑。
9. 使用 GPU 进行深度学习，往往能够把深度学习算法的训练时间，缩短一个，乃至两个数量级。
10. GPU 现在也越来越多地用在各种科学计算和机器学习上，而不仅仅是用在图形渲染上了。

## 32 | FPGA、ASIC 和 TPU（上）：计算机体系结构的黄金时代

1.  FPGA（`Field-Programmable Gate Array）`）

    - 现场可编程门阵列
    - FPGA 本质上是一个可以通过编程，来控制硬件电路的芯片
    - 通过用 LUT 这样的存储设备，来代替需要的硬连线的电路，有了可编程的逻辑门，然后把很多 LUT 和寄存器放在一起，变成一个更复杂的逻辑电路，也就是 CLB，然后通过控制可编程布线中的很多开关，最终设计出属于我们自己的芯片功能
    - FPGA，常常被我们用来进行芯片的设计和验证工作，也可以直接拿来当成专用的芯片，替换掉 CPU 或者 GPU，以节约成本

2.  ASIC
    - 是针对特定的使用场景设计出来的芯片，比如，摄像头、音频、“挖矿”或者深度学习。
    - ASIC 的研发成本高昂，但是生产制造成本和能耗都很低
    - 对于有大量需求的专用芯片，用 ASIC 是很划得来的

## 33 | 解读 TPU：设计和拆解一块 ASIC 芯片

1. 第一代 TPU，是为了做各种深度学习的推断而设计出来的
2. TPU 并不需要太灵活的可编程能力，只要能够迭代完成常见的深度学习推断过程中一层的计算过程就好了
3. TPU 的硬件构造里面，把矩阵乘法、累加器和激活函数都做成了对应的专门的电路。
4. TPU 采用了现有的 PCI-E 接口，可以和 GPU 一样直接插在主板上，并且采用了作为一个没有取指令功能的协处理器，就像 387 之于 386 一样，仅仅用来进行需要的各种运算。
5. 因为机器学习的推断功能，通常做了数值的归一化，所以对于矩阵乘法的计算精度要求有限，整个矩阵乘法的计算模块采用了 8 Bits 来表示浮点数，而不是像 Intel CPU 里那样用上了 32 Bits。

## 34 | 理解虚拟机：你在云上拿到的计算机是什么样的？

1. 我们现在的云服务平台上，你能够租到的服务器其实都是虚拟机，而不是物理机
2. 正是虚拟机技术的出现，使得整个云服务生态得以出现
3. 虚拟机是模拟一个计算机系统的技术，而其中最简单的办法叫模拟器
4. 我们日常在 PC 上进行 Android 开发，其实就是在使用这样的模拟器技术。不过模拟器技术在性能上实在不行，所以我们才有了虚拟化这样的技术。
5. 在宿主机的操作系统上，运行一个虚拟机监视器，然后再在虚拟机监视器上运行客户机的操作系统，这就是现代的虚拟化技术。
6. 虚拟化技术可以分成 Type-1 和 Type-2 这两种类型。
7. Type-1 类型的虚拟化机，实际的指令不需要再通过宿主机的操作系统，而可以直接通过虚拟机监视器访问硬件，所以性能比 Type-2 要好
8. 而 Type-2 类型的虚拟机，所有的指令需要经历客户机操作系统、虚拟机监视器、宿主机操作系统，所以性能上要慢上不少。不过因为经历了宿主机操作系统的一次“翻译”过程，它的硬件兼容性往往会更好一些。
9. 我们常常在同一个物理机上，跑上 8 个、10 个的虚拟机。而且这些虚拟机的操作系统，其实都是同一个 Linux Kernel 的版本。于是，轻量级的 Docker 技术就进入了我们的视野
10. Docker 也被很多人称之为“操作系统级”的虚拟机技术。不过 Docker 并没有再单独运行一个客户机的操作系统，而是直接运行在宿主机操作系统的内核之上
11. Docker 也是现在流行的微服务架构底层的基础设施。

## 35 | 存储器层次结构全景：数据存储的大金字塔长什么样？

1. CPU 比喻成高速运转的大脑，那么和大脑同步的寄存器（Register），就存放着我们当下正在思考和处理的数据。
2. 而 L1-L3 的 CPU Cache，好比存放在我们大脑中的短期到长期的记忆。我们需要小小花费一点时间，就能调取并进行处理。
3. 从寄存器、CPU Cache，到内存、硬盘，这样一层层下来的存储器，速度越来越慢，空间越来越大，价格也越来越便宜。
4. SSD（Solid-state drive 或 Solid-statedisk，固态硬盘） 这种基于 NAND 芯片的高速硬盘，
5. HDD（Hard Disk Drive，硬盘）传统硬件，它的访问速度受限于它的物理结构，是最慢的

## 36 | 局部性原理：数据库性能跟不上，加个缓存就好了？

1. 局部性是时间局部性，就是我们最近访问过的数据还会被反复访问。
2. 局部性是空间局部性，就是我们最近访问过数据附近的数据很快会被访问到。
3. 通过将热点数据加载并保留在速度更快的存储设备里面，我们可以用更低的成本来支撑服务器。
4. 访问存储器的性能问题的时候，是否可以简单地添加一层数据缓存

## 37 | 理解 CPU Cache（上）：“4 毫秒”究竟值多少钱？

1. 程序的性能瓶颈，来自使用 DRAM 芯片的内存访问速度
2. 根据摩尔定律，自上世纪 80 年代以来，CPU 和内存的性能鸿沟越拉越大。
3. 现代 CPU 的设计者们，直接在 CPU 中嵌入了使用更高性能的 SRAM 芯片的 Cache，来弥补这一性能差异。
4. 通过巧妙地将内存地址，拆分成“索引 + 组标记 + 偏移量”的方式，使得我们可以将很大的内存地址，映射到很小的 CPU Cache 地址里
5. 而 CPU Cache 带来的毫秒乃至微秒级别的性能差异，又能带来巨大的商业利益，十多年前的高频交易行业就是最好的例子。

## 38 | 高速缓存（下）：你确定你的数据更新了么？

1. 在 CPU Cache 里，对于数据的写入，我们也有写直达和写回这两种解决方案
2. 写直达把所有的数据都直接写入到主内存里面，简单直观，但是性能就会受限于内存的访问速度。
3. 而写回则通常只更新缓存，只有在需要把缓存里面的脏数据交换出去的时候，才把数据同步到主内存里。在缓存经常会命中的情况下，性能更好。

## 39 | MESI 协议：如何让多核 CPU 的高速缓存保持一致？

1. MESI
   - M：代表已修改（Modified）
   - E：代表独占（Exclusive）
   - S：代表共享（Shared）
   - I：代表已失效（Invalidated）
2. MESI 协议是一种基于写失效的缓存一致性协议。写失效的协议的好处是，我们不需要在总线上传输数据内容，而只需要传输操作信号和地址信号就好了，不会那么占总线带宽。
3. 整个 MESI 的状态变更，则是根据来自自己 CPU 核心的请求，以及来自其他 CPU 核心通过总线传输过来的操作信号和地址信息，进行状态流转的一个有限状态机。

## 40 | 理解内存（上）：虚拟内存和内存保护是什么？

1. 多级页表就像是一颗树。因为一个进程的内存地址相对集中和连续，所以采用这种页表树的方式，可以大大节省页表所需要的空间。
2. 而因为每个进程都需要一个独立的页表，这个空间的节省是非常可观的。

## 41 丨理解内存（下）：解析 TLB 和内存保护

1. 多级页表虽然节省空间了，却要花费更多的时间去多次访问内存
2. 我们在实际进行地址转换的 MMU 旁边放上了 TLB 这个用于地址转换的缓存。
3. TLB 也像 CPU Cache 一样，分成指令和数据部分，也可以进行 L1、L2 这样的分层。
4. 为了防止因为各种漏洞，导致一个进程可以访问别的进程的数据或者代码，甚至是执行对应的代码，造成严重的安全问题，我们介绍了最常用的两个内存保护措施，可执行空间保护和地址空间
   布局随机化。
5. 通过让数据空间里面的内容不能执行，可以避免了类似于“注入攻击”的攻击方式。
6. 通过随机化内存空间的分配，可以避免让一个进程的内存里面的代码，被推测出来，从而不容易被攻击。

## 42 | 总线：计算机内部的高速公路

1. 总线: 计算机里各个不同的组件之间用来通信的渠道
2. 总线的设计思路，核心是为了减少多个模块之间交互的复杂性和耦合度
3. 事件总线就是我们常见的一个设计模式，通常事件总线也会和订阅者发布者模式结合起来，成为大型系统的各个松耦合的模块之间交互的一种主要模式。
4. 在实际的硬件层面，总线其实就是一组连接电路的线路
5. 因为不同设备之间的速度有差异，所以一台计算机里面往往会有多个总线。
6. 常见的就有在 CPU 内部和高速缓存通信的本地总线，以及和外部 I/O 设备以及内存通信的前端总线。
7. 前端总线通常也被叫作系统总线。它可以通过一个 I/O 桥接器，拆分成两个总线，分别来和 I/O 设备以及内存通信。
8. 这样拆开的两个总线，就叫作 I/O 总线和内存总线
9. 总线本身的电路功能，又可以拆分成用来传输数据的数据线、用来传输地址的地址线，以及用来传输控制信号的控制线。

## 43 | 输入输出设备：我们并不是只能用灯泡显示“0”和“1”

1. 在 I/O 设备这一侧，我们把 I/O 设备拆分成，能和 CPU 通信的接口电路，以及实际的 I/O 设备本身。
2. 接口电路里面有对应的状态寄存器、命令寄存器、数据寄存器、数据缓冲区和设备内存等等。
3. 接口电路通过总线和 CPU 通信，接收来自 CPU 的指令和数据。
4. 接口电路中的控制电路，再解码接收到的指令，实际去操作对应的硬件设备。
5. 对 CPU 来说，它看到的并不是一个个特定的设备，而是一个个内存地址或者端口地址
6. CPU 只是向这些地址传输数据或者读取数据。所需要的指令和操作内存地址的指令其实没有什么本质差别
7. 通过软件层面对于传输的命令数据的定义，而不是提供特殊的新的指令，来实际操作对应的 I/O 硬件。

## 44 | 理解 IO_WAIT：I/O 性能到底是怎么回事儿？

1. 可以通过 `as ssd` 这样的性能评测软件，看一看自己的硬盘性能
2. 硬盘的两个核心指标，响应时间和数据传输率
3. 评测的标准是每秒钟能够进行输入输出的操作次数，也就是 `IOPS` 这个核心性能指标
4. 使用 `PCI Express` 接口的 `SSD` 硬盘，`IOPS` 也就只是到了 2 万左右
5. `CPU` 的每秒 20 亿次操作的能力，程序对外响应慢，其实都是 `CPU` 在等待 `I/O` 操作完成
6. 在 `Linux` 下，我们可以通过 `top` 这样的命令，来看整个服务器的整体负载
7. 通过 `iostat` 这个命令，来看到各个硬盘这个时候的读写情况
8. `iotop` 这个命令，能够帮助我们定位到到底是哪一个进程在进行大量的 `I/O` 操作。

## 45 | 机械硬盘：Google 早期用过的“黑科技”

1. 机械硬盘的硬件，主要由盘面、磁头和悬臂三部分组成
2. 我们的数据在盘面上的位置，可以通过磁道、扇区和柱面来定位
3. 实际的一次对于硬盘的访问，需要把盘面旋转到某一个“几何扇区”，对准悬臂的位置
4. 然后，悬臂通过寻道，把磁头放到我们实际要读取的扇区上。
5. 受制于机械硬盘的结构，我们对于随机数据的访问速度，就要包含旋转盘面的平均延时和移动悬臂的寻道时间。
6. 通过这两个时间，我们能计算出机械硬盘的 IOPS。
7. 7200 转机械硬盘的 IOPS，只能做到 100 左右。
8. 在互联网时代的早期，我们也没有 `SSD` 硬盘可以用，所以工程师们就想出了 `Partial Stroking` 这个浪费存储空间，但是可以缩短寻道时间来提升硬盘的 `IOPS` 的解决方案。

## 46 | SSD 硬盘（上）：如何完成性能优化的 KPI？

1. `SSD` 的物理原理，也就是“电容 + 电压计”的组合
2. `SSD` 硬盘的物理构造，也就是裸片、平面、块、页的层次结构。
3. 我们对于数据的写入，只能是一页一页的，不能对页进行覆写，对于数据的擦除，只能整块进行
4. 我们需要用一个，类似“磁盘碎片整理”或者“内存垃圾回收”这样的机制，来清理块当中的数据空洞。
5. `SSD` 硬盘也会保留一定的预留空间，避免出现硬盘无法写满的情况。
6. `SSD` 硬盘的使用寿命受限于可以擦除的次数。
7. `SSD` 硬盘，特别适合读多写少的应用，我们的系统盘适合用 `SSD`，不能用作下载盘
8. 在数据中心里面，我们拿 `SSD` 硬盘用来做数据库，存放电商网站的商品信息很合适。
9. 用来作为 `Hadoop` 这样的 `Map-Reduce` 应用的数据盘就不行了。因为 `Map-Reduce` 任务会大量在任务中间向硬盘写入中间数据再删
   除掉，这样用不了多久，`SSD` 硬盘的寿命就会到了。

## 47 | SSD 硬盘（下）：如何完成性能优化的 KPI？

1. 因为 `SSD` 硬盘的使用寿命，受限于块的擦除次数，所以我们需要通过一个磨损均衡的策略，来管理 `SSD` 硬盘的各个块的擦除次数。
2. 我们通过在逻辑块地址和物理块地址之间，引入 `FTL` 这个映射层，使得操作系统无需关心物理块的擦写次数，而是由 `FTL` 里的软件算法，来协调到底每一次写入应该磨损哪一块。
3. 除了磨损均衡之外，操作系统和 `SSD` 硬件的特性还有一个不匹配的地方。那就是，操作系统在删除数据的时候，并没有真的删除物理层面的数据，而只是修改了 `inode` 里面的数据。这个“伪删除”，使得 `SSD` 硬盘在逻辑和物理层面，都没有意识到有些块其实已经被删除了。这就导致在垃圾回收的时候，会浪费很多不必要的读写资源。
4. SSD 这个需要进行垃圾回收的特性，使得我们在写入数据的时候，会遇到写入放大。明明我们只是写入了 4MB 的数据，可能在 SSD 的硬件层面，实际写入了 8MB、16MB 乃至更多的数据。
5. 针对这些特性，`AeroSpike`，这个专门针对 SSD 硬盘特性的 KV 数据库，设计了很多的优化点，包括跳过文件系统直写硬盘、写大块读小块、用高水位算法持续进行磁盘碎片整理，以及只使用 SSD 硬盘的一半空间

## 48 丨 DMA：为什么 Kafka 这么快？

1. 如果我们始终让 `CPU` 来进行各种数据传输工作，会特别浪费。一方面，我们的数据传输工作用不到多少 `CPU` 核心的“计算”功能。另一方面，`CPU` 的运转速度也比 `I/O` 操作要快很多。所以，我们希望能够给 `CPU`“减负”。
2. 工程师们就在主板上放上了 `DMAC` 这样一个协处理器芯片。通过这个芯片，`CPU`只需要告诉 `DMAC`，我们要传输什么数据，从哪里来，到哪里去，就可以放心离开了。后续的实际数据传输工作，都会有 `DMAC` 来完成。
3. 随着现代计算机各种外设硬件越来越多，光一个通用的 `DMAC` 芯片不够了，我们在各个外设上都加上了 `DMAC` 芯片，使得 `CPU` 很少再需要关心数据传输的工作了。
4. 在我们实际的系统开发过程中，利用好 `DMA` 的数据传输机制，也可以大幅提升 `I/O` 的吞吐率。最典型的例子就是 `Kafka`。
5. 传统地从硬盘读取数据，然后再通过网卡上向外发送，我们需要进行四次数据传输，其中有两次是发生在内存里的缓冲区和对应的硬件设备之间，我们没法节省掉。但是还有两次，完
   全是通过 `CPU` 在内存里面进行数据复制。
6. 在 `Kafka` 里，通过 `Java` 的 `NIO` 里面 `FileChannel` 的 `transferTo` 方法调用，我们可以不用把数据复制到我们应用程序的内存里面。通过 `DMA` 的方式，我们可以把数据从内存缓冲区直接写到网卡的缓冲区里面。在使用了这样的零拷贝的方法之后呢，我们传输同样数据的时间，可以缩减为原来的 `1/3`，相当于提升了 `3` 倍的吞吐率。
7. `Kafka` 是目前实时数据传输管道的标准解决方案

## 49 | 数据完整性（上）：硬件坏了怎么办？

1. 硬件带来的错误，其实我们没有办法在软件层面解决。
2. 奇偶校验，也就是如何通过冗余的一位数据，发现在硬件层面出现的位错误。

## 50 丨数据完整性（下）：如何还原犯罪现场？

1. 通过在数据中添加多个冗余的校验码位，海明码不仅能够检测到数据中的错误，还能够在只有单个位的数据出错的时候，把错误的一位纠正过来。
2. 实际的海明码编码的过程也并不复杂，我们通过用不同过的校验位，去匹配多个不同的数据组，确保任何一个数据位出错，都会产生一个多个校验码位出错的唯一组合
3. 这样，在出错的时候，我们就可以反过来找到出错的数据位，并纠正过来。当只有一个校验码位出错的时候，我们就知道实际出错的是校验码位了。
