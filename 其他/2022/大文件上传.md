# 大文件上传

## 1.切片

1. 文件`file`继承至`Blob`，可直接切片

```js
// 生成文件切片
const createFileChunk = (file, size = 1024) => {
    const fileChunkList = [];
    let cur = 0;
    while (cur < file.size) {
        fileChunkList.push({ file: file.slice(cur, cur + size) });
        cur += size;
    }
    return fileChunkList;
};
```

## 2.上传进度

1. `xhr`支持原生的`onProgress`方法

```js
// 用闭包保存每个 chunk 的进度数据
const createProgressHandler = (item) => {
    return (e) => {
        item.percentage = parseInt(String((e.loaded / e.total) * 100));
    };
};
// 单个文件的进度
xhr.upload.onprogress = createProgressHandler(this.data[index]);
```

## 3.暂停

1. 通过`xhr.abort`暂停

## 4.切片 hash

1. 通过 `md5` 对切片文件的内容生成 `hash` 值
2. `hash`耗时计算放到`worker`中进行或者`requestIdleCallback`中
3. 无需取整个文件，取首、中、尾各 5 个字节计算

## 5.秒传/断点续传

1. 后端校验文件的`hash`已经上传过，直接返回成功

## 6.服务器-切片合并

1. 根据下标排序，再使用`pipeStream`的方式合并

```js
// 合并切片
const mergeFileChunk = async (filePath, fileHash, size) => {
    const chunkDir = path.resolve(UPLOAD_DIR, fileHash);
    const chunkPaths = await fse.readdir(chunkDir);
    // 根据切片下标进行排序
    // 否则直接读取目录的获得的顺序可能会错乱
    chunkPaths.sort((a, b) => a.split("-")[1] - b.split("-")[1]);
    await Promise.all(
        chunkPaths.map((chunkPath, index) =>
            pipeStream(
                path.resolve(chunkDir, chunkPath),
                // 指定位置创建可写流
                fse.createWriteStream(filePath, {
                    start: index * size,
                    end: (index + 1) * size,
                })
            )
        )
    );
    fse.rmdirSync(chunkDir); // 合并后删除保存切片的目录
};

const pipeStream = (path, writeStream) =>
    new Promise((resolve) => {
        const readStream = fse.createReadStream(path);
        readStream.on("end", () => {
            fse.unlinkSync(path);
            resolve();
        });
        readStream.pipe(writeStream);
    });
```

## 7.上传控制请求数量，避免大文件浏览器卡死

## 8.参考 TCP 拥塞控制原理，调整切片大小和网速匹配

-   比如我们理想是 30 秒传递一个
-   初始大小定为 1M，如果上传花了 10 秒，那下一个区块大小变成 3M
-   如果上传花了 60 秒，那下一个区块大小变成 500KB 以此类推

## 9.catch 后重试 3 次

## 10.未 merge 的切片定期清理
